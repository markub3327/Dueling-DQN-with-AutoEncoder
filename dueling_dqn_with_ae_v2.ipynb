{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN with AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%env WANDB_AGENT_MAX_INITIAL_FAILURES=1024\n",
    "\n",
    "import wandb\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"score\"},\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"value\": 2000},\n",
    "        \"buffer_size\": {\"value\": 1000000},\n",
    "        \"batch_size\": {\"value\": 256},\n",
    "        \"lr\": {\"value\": 3e-4},\n",
    "        \"global_clipnorm\": {\"value\": 1.0},\n",
    "        \"tau\": {\"value\": 0.01},\n",
    "        \"gamma\": {\"value\": 0.99},\n",
    "        \"temp_init\": {\"value\": 1.0},\n",
    "        \"temp_min\": {\"value\": 0.01},\n",
    "        \"temp_decay\": {\"value\": 1e-5},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Dueling-DQN-with-AutoEncoder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(tf.keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(\n",
    "            512,\n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "        )\n",
    "        self.fc2 = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "        )\n",
    "        self.V = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(0.01),\n",
    "        )\n",
    "        self.A = tf.keras.layers.Dense(\n",
    "            action_space,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(0.01),\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc1(inputs, training=training)\n",
    "        x = self.fc2(x, training=training)\n",
    "        V = self.V(x, training=training)\n",
    "        A = self.A(x, training=training)\n",
    "        adv_mean = tf.reduce_mean(A, axis=-1, keepdims=True)\n",
    "        return V + (A - adv_mean)\n",
    "\n",
    "    def get_action(self, state, temperature):\n",
    "        return tf.random.categorical(self(state) / temperature, 1)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class IntrinsicModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(IntrinsicModel, self).__init__()\n",
    "\n",
    "        # init reward normalization\n",
    "        self.rew_rms = tf.keras.layers.Normalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Encoder\n",
    "        self._encoder = [\n",
    "            tf.keras.layers.Dense(\n",
    "                128,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                64,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                32,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                16,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            # Latent space\n",
    "            tf.keras.layers.Dense(\n",
    "                (input_shape[-1] // 2),\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "                name=\"latent_space\",\n",
    "            ),\n",
    "        ]\n",
    "        # Decoder\n",
    "        self._decoder = [\n",
    "            tf.keras.layers.Dense(\n",
    "                16,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                32,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                64,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                128,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                input_shape[-1],\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "                name=\"reconstruction\",\n",
    "            ),\n",
    "        ]\n",
    "        self.rew_rms.build((None, 1))\n",
    "        super(IntrinsicModel, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, only_encoder=None):\n",
    "        for l in self._encoder:\n",
    "            inputs = l(inputs, training=training)\n",
    "        if not only_encoder:\n",
    "            for l in self._decoder:\n",
    "                inputs = l(inputs, training=training)\n",
    "        return inputs\n",
    "\n",
    "    def get_int_reward(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=self.dtype)\n",
    "        y_pred = self(inputs, training=False)\n",
    "        reward = tf.reduce_sum(tf.square(inputs - y_pred), axis=-1, keepdims=True)\n",
    "        return reward\n",
    "    \n",
    "    def update_reward_norm(self, reward):\n",
    "        self.rew_rms.update_state(reward)\n",
    "        self.rew_rms.finalize_state()\n",
    "\n",
    "    def normalize_reward(self, reward):\n",
    "        return tf.nn.relu6(self.rew_rms(reward))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, shape, size=1e6):\n",
    "        self.size = int(size)\n",
    "        self.counter = 0\n",
    "        self.state_buffer = np.zeros((self.size, *shape), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros(self.size, dtype=np.int32)\n",
    "        self.ext_reward_buffer = np.zeros(self.size, dtype=np.float32)\n",
    "        self.new_state_buffer = np.zeros((self.size, *shape), dtype=np.float32)\n",
    "        self.terminal_buffer = np.zeros(self.size, dtype=np.bool_)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counter\n",
    "\n",
    "    def add(self, state, action, ext_reward, new_state, done):\n",
    "        idx = self.counter % self.size\n",
    "        self.state_buffer[idx] = state\n",
    "        self.action_buffer[idx] = action\n",
    "        self.ext_reward_buffer[idx] = ext_reward\n",
    "        self.new_state_buffer[idx] = new_state\n",
    "        self.terminal_buffer[idx] = done\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_buffer = min(self.counter, self.size)\n",
    "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
    "        state_batch = self.state_buffer[batch]\n",
    "        action_batch = self.action_buffer[batch]\n",
    "        ext_reward_batch = self.ext_reward_buffer[batch]\n",
    "        new_state_batch = self.new_state_buffer[batch]\n",
    "        done_batch = self.terminal_buffer[batch]\n",
    "\n",
    "        return (\n",
    "            state_batch,\n",
    "            action_batch,\n",
    "            ext_reward_batch,\n",
    "            new_state_batch,\n",
    "            done_batch,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(net, net_targ, tau):\n",
    "    for source_weight, target_weight in zip(\n",
    "        net.trainable_variables, net_targ.trainable_variables\n",
    "    ):\n",
    "        target_weight.assign(tau * source_weight + (1.0 - tau) * target_weight)\n",
    "\n",
    "\n",
    "def train_step(dqn, target_dqn, int_model, replay_buffer, batch_size, tau, gamma):\n",
    "    (\n",
    "        states,\n",
    "        actions,\n",
    "        ext_rewards,\n",
    "        next_states,\n",
    "        dones,\n",
    "    ) = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # predict next Q\n",
    "    next_Q = target_dqn(next_states)\n",
    "    next_Q = tf.reduce_max(next_Q, axis=-1)\n",
    "\n",
    "    # update int model\n",
    "    int_loss = int_model.train_on_batch(states, states)\n",
    "    int_rewards = int_model.get_int_reward(next_states)\n",
    "    int_model.update_reward_norm(int_rewards)\n",
    "    int_rewards = int_model.normalize_reward(int_rewards)\n",
    "\n",
    "    # get targets\n",
    "    targets = np.array(dqn(states))\n",
    "    # for experiments with only extrinsic reward, use 'ext_rewards' instead of 'int_rewards'\n",
    "    targets[np.arange(batch_size), actions] = int_rewards[:, 0] + (\n",
    "        (1.0 - tf.cast(dones, dtype=tf.float32)) * gamma * next_Q\n",
    "    )\n",
    "\n",
    "    # update dqn\n",
    "    dqn_loss = dqn.train_on_batch(states, targets)\n",
    "\n",
    "    # soft update target Q\n",
    "    update_target(dqn, target_dqn, tau=tau)\n",
    "\n",
    "    return dqn_loss, int_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # make an environment\n",
    "        # env = gymnasium.make(\"CartPole-v1\")\n",
    "        # env = gymnasium.make('MountainCar-v0')\n",
    "        env = gymnasium.make(\"LunarLander-v2\")\n",
    "        # env = gymnasium.make('Acrobot-v1')\n",
    "\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "\n",
    "        # init variables\n",
    "        total_steps = 0\n",
    "        temp = config.temp_init\n",
    "\n",
    "        # init models\n",
    "        q_model = DuelingDQN(action_space)\n",
    "        q_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=config.lr,\n",
    "                global_clipnorm=config.global_clipnorm,\n",
    "            ),\n",
    "            loss=\"logcosh\",\n",
    "        )\n",
    "        target_q_model = DuelingDQN(action_space)\n",
    "        target_q_model.set_weights(q_model.get_weights())\n",
    "        int_model = IntrinsicModel()\n",
    "        int_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=config.lr,\n",
    "                global_clipnorm=config.global_clipnorm,\n",
    "            ),\n",
    "            loss=\"mse\",\n",
    "        )\n",
    "\n",
    "        # init buffer\n",
    "        exp_buffer = ReplayBuffer(shape=env.observation_space.shape, size=config.buffer_size)\n",
    "\n",
    "        # play\n",
    "        for epoch in range(0, config.epochs):\n",
    "            state, _ = env.reset()\n",
    "            done, truncated = False, False\n",
    "            ep_ext_reward, ep_int_reward, ep_step = 0, 0, 0\n",
    "\n",
    "            while (not done) and (not truncated):\n",
    "                action = q_model.get_action(\n",
    "                    tf.expand_dims(state, axis=0),\n",
    "                    temp,\n",
    "                )\n",
    "                action = np.array(action, copy=False, dtype=env.env.action_space.dtype)\n",
    "\n",
    "                next_state, ext_reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                # get intrinsic reward\n",
    "                int_reward = int_model.get_int_reward(\n",
    "                    tf.expand_dims(next_state, axis=0)\n",
    "                )\n",
    "                int_reward = int_model.normalize_reward(int_reward)\n",
    "\n",
    "                exp_buffer.add(state, action, ext_reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "                ep_step += 1\n",
    "                ep_ext_reward += ext_reward\n",
    "                ep_int_reward += int_reward[0, 0]\n",
    "\n",
    "                # decrement temperature\n",
    "                temp -= config.temp_decay\n",
    "                temp = max(config.temp_min, temp)\n",
    "\n",
    "                if len(exp_buffer) >= config.batch_size:\n",
    "                    dqn_loss, int_loss = train_step(\n",
    "                        q_model,\n",
    "                        target_q_model,\n",
    "                        int_model,\n",
    "                        exp_buffer,\n",
    "                        config.batch_size,\n",
    "                        config.tau,\n",
    "                        config.gamma,\n",
    "                    )\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"dqn_loss\": dqn_loss,\n",
    "                            \"int_loss\": int_loss,\n",
    "                        },\n",
    "                        commit=False,\n",
    "                    )\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"score\": ep_ext_reward,\n",
    "                    \"int_score\": ep_int_reward,\n",
    "                    \"steps\": ep_step,\n",
    "                    \"temperature\": temp,\n",
    "                },\n",
    "                commit=True,\n",
    "                step=epoch,\n",
    "            )\n",
    "            print(\n",
    "                f\"Int reward - Mean: {int_model.rew_rms.mean}, Stddev: {tf.sqrt(int_model.rew_rms.variance)}, Count: {int_model.rew_rms.count}\"\n",
    "            )\n",
    "\n",
    "            # For 'Acrobot-v1' the threshold is -92\n",
    "            # For 'MountainCar-v0' the threshold is 'env.spec.reward_threshold'\n",
    "            # For 'CartPole-v1' the threshold is 'env.spec.reward_threshold'\n",
    "            # For 'LunarLander-v2' the threshold is 'env.spec.reward_threshold'\n",
    "            if ep_ext_reward > env.spec.reward_threshold:\n",
    "                q_model.summary()\n",
    "                target_q_model.summary()\n",
    "                int_model.summary()\n",
    "\n",
    "                print(\n",
    "                    f\"\\n{env.spec.id} is sloved! {(epoch+1)} Episode in {total_steps} steps with reward {ep_ext_reward}\"\n",
    "                )\n",
    "\n",
    "                # show latent space\n",
    "                latent_space = state_space // 2\n",
    "                sampled_states, _, _, _, _ = exp_buffer.sample(ep_step)\n",
    "                code_normal = int_model(sampled_states, only_encoder=True)\n",
    "                code_anomaly = int_model(\n",
    "                    exp_buffer.new_state_buffer[\n",
    "                        len(exp_buffer) - ep_step : len(exp_buffer)\n",
    "                    ],\n",
    "                    only_encoder=True,\n",
    "                )\n",
    "                if latent_space > 2:\n",
    "                    t_sne = TSNE(n_components=2)\n",
    "                    code_normal = t_sne.fit_transform(code_normal)\n",
    "\n",
    "                    t_sne = TSNE(n_components=2)\n",
    "                    code_anomaly = t_sne.fit_transform(code_anomaly)\n",
    "\n",
    "                    data = [\n",
    "                        [x, y, \"Normal\"]\n",
    "                        for (x, y) in zip(code_normal[:, 0], code_normal[:, 1])\n",
    "                    ]\n",
    "                    data += [\n",
    "                        [x, y, \"Winner\"]\n",
    "                        for (x, y) in zip(code_anomaly[:, 0], code_anomaly[:, 1])\n",
    "                    ]\n",
    "                elif latent_space == 2:\n",
    "                    data = [\n",
    "                        [x, y, \"Normal\"]\n",
    "                        for (x, y) in zip(code_normal[:, 0], code_normal[:, 1])\n",
    "                    ]\n",
    "                    data += [\n",
    "                        [x, y, \"Winner\"]\n",
    "                        for (x, y) in zip(code_anomaly[:, 0], code_anomaly[:, 1])\n",
    "                    ]\n",
    "                else:\n",
    "                    data = [\n",
    "                        [x, y, \"Normal\"]\n",
    "                        for (x, y) in zip(\n",
    "                            code_normal[:, 0], np.ones_like(code_normal[:, 0])\n",
    "                        )\n",
    "                    ]\n",
    "                    data += [\n",
    "                        [x, y, \"Winner\"]\n",
    "                        for (x, y) in zip(\n",
    "                            code_anomaly[:, 0], np.ones_like(code_anomaly[:, 0])\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                table = wandb.Table(data=data, columns=[\"LS 1\", \"LS 2\", \"Type\"])\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"Latent space\": wandb.plot.scatter(table, \"LS 1\", \"LS 2\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                break\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, run, count=100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f8d278994ad275c52e282a0ce8b95e2077469e5ea30fad259438b075059602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
