{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN with AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWnm3qot3o1W"
   },
   "outputs": [],
   "source": [
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%env WANDB_AGENT_MAX_INITIAL_FAILURES=1024\n",
    "\n",
    "import wandb\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1534482400648,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "maRVADiTlzHD",
    "outputId": "783b7610-95c2-4b54-b2ce-d8e853c484ba"
   },
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"score\"},\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"value\": 2000},\n",
    "        \"buffer_size\": {\"value\": 1000000},\n",
    "        \"batch_size\": {\"value\": 256},\n",
    "        \"lr\": {\"value\": 3e-4},\n",
    "        \"global_clipnorm\": {\"value\": 1.0},\n",
    "        \"tau\": {\"value\": 0.01},\n",
    "        \"gamma\": {\"value\": 0.99},\n",
    "        \"temp_init\": {\"value\": 1.0},\n",
    "        \"temp_min\": {\"value\": 0.01},\n",
    "        \"temp_decay\": {\"value\": 1e-5},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Dueling-DQN-with-AutoEncoder\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(tf.keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(\n",
    "            512,\n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "        )\n",
    "        self.fc2 = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "        )\n",
    "        self.V = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(0.01),\n",
    "        )\n",
    "        self.A = tf.keras.layers.Dense(\n",
    "            action_space,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.Orthogonal(0.01),\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc1(inputs, training=training)\n",
    "        x = self.fc2(x, training=training)\n",
    "        V = self.V(x, training=training)\n",
    "        A = self.A(x, training=training)\n",
    "        adv_mean = tf.reduce_mean(A, axis=-1, keepdims=True)\n",
    "        return V + (A - adv_mean)\n",
    "\n",
    "    def get_action(self, state, temperature):\n",
    "        return tf.random.categorical(self(state) / temperature, 1)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ffkl_5C4R81"
   },
   "outputs": [],
   "source": [
    "class IntrinsicModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(IntrinsicModel, self).__init__()\n",
    "\n",
    "        # init reward normalization\n",
    "        self.rew_rms = tf.keras.layers.Normalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Encoder\n",
    "        self._encoder = [\n",
    "            tf.keras.layers.Dense(\n",
    "                128,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                64,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                32,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                16,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            # Latent space\n",
    "            tf.keras.layers.Dense(\n",
    "                (input_shape[-1] // 2),\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "                name=\"latent_space\",\n",
    "            ),\n",
    "        ]\n",
    "        # Decoder\n",
    "        self._decoder = [\n",
    "            tf.keras.layers.Dense(\n",
    "                16,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                32,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                64,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                128,\n",
    "                activation=\"elu\",\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "            ),\n",
    "            tf.keras.layers.Dense(\n",
    "                input_shape[-1],\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.keras.initializers.Orthogonal(tf.sqrt(2.0)),\n",
    "                name=\"reconstruction\",\n",
    "            ),\n",
    "        ]\n",
    "        self.rew_rms.build((None, 1))\n",
    "        super(IntrinsicModel, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None, only_encoder=None):\n",
    "        for l in self._encoder:\n",
    "            inputs = l(inputs, training=training)\n",
    "        if not only_encoder:\n",
    "            for l in self._decoder:\n",
    "                inputs = l(inputs, training=training)\n",
    "        return inputs\n",
    "\n",
    "    def get_int_reward(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=self.dtype)\n",
    "        y_pred = self(inputs, training=False)\n",
    "        reward = tf.reduce_sum(tf.square(inputs - y_pred), axis=-1, keepdims=True)\n",
    "\n",
    "        # update reward statistics\n",
    "        self.rew_rms.update_state(reward)\n",
    "        self.rew_rms.finalize_state()\n",
    "\n",
    "        # normalize intrinsic reward\n",
    "        reward = tf.nn.relu6(self.rew_rms(reward))\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, shape, size=1e6):\n",
    "        self.size = int(size)\n",
    "        self.counter = 0\n",
    "        self.state_buffer = np.zeros((self.size, *shape), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros(self.size, dtype=np.int32)\n",
    "        self.int_reward_buffer = np.zeros(self.size, dtype=np.float32)\n",
    "        self.ext_reward_buffer = np.zeros(self.size, dtype=np.float32)\n",
    "        self.new_state_buffer = np.zeros((self.size, *shape), dtype=np.float32)\n",
    "        self.terminal_buffer = np.zeros(self.size, dtype=np.bool_)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counter\n",
    "\n",
    "    def add(self, state, action, ext_reward, int_reward, new_state, done):\n",
    "        idx = self.counter % self.size\n",
    "        self.state_buffer[idx] = state\n",
    "        self.action_buffer[idx] = action\n",
    "        self.int_reward_buffer[idx] = int_reward\n",
    "        self.ext_reward_buffer[idx] = ext_reward\n",
    "        self.new_state_buffer[idx] = new_state\n",
    "        self.terminal_buffer[idx] = done\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_buffer = min(self.counter, self.size)\n",
    "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
    "        state_batch = self.state_buffer[batch]\n",
    "        action_batch = self.action_buffer[batch]\n",
    "        int_reward_batch = self.int_reward_buffer[batch]\n",
    "        ext_reward_batch = self.ext_reward_buffer[batch]\n",
    "        new_state_batch = self.new_state_buffer[batch]\n",
    "        done_batch = self.terminal_buffer[batch]\n",
    "\n",
    "        return (\n",
    "            state_batch,\n",
    "            action_batch,\n",
    "            ext_reward_batch,\n",
    "            int_reward_batch,\n",
    "            new_state_batch,\n",
    "            done_batch,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(net, net_targ, tau):\n",
    "    for source_weight, target_weight in zip(\n",
    "        net.trainable_variables, net_targ.trainable_variables\n",
    "    ):\n",
    "        target_weight.assign(tau * source_weight + (1.0 - tau) * target_weight)\n",
    "\n",
    "\n",
    "def train_step(dqn, target_dqn, int_model, replay_buffer, batch_size, tau, gamma):\n",
    "    (\n",
    "        states,\n",
    "        actions,\n",
    "        ext_rewards,\n",
    "        int_rewards,\n",
    "        next_states,\n",
    "        dones,\n",
    "    ) = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # predict next Q\n",
    "    next_Q = target_dqn(next_states)\n",
    "    next_Q = tf.reduce_max(next_Q, axis=-1)\n",
    "\n",
    "    # get targets\n",
    "    targets = np.array(dqn(states))\n",
    "    # for experiments with only extrinsic reward, use 'ext_rewards' instead of 'int_rewards'\n",
    "    targets[np.arange(batch_size), actions] = int_rewards + (\n",
    "        (1.0 - tf.cast(dones, dtype=tf.float32)) * gamma * next_Q\n",
    "    )\n",
    "\n",
    "    # update dqn\n",
    "    dqn_loss = dqn.train_on_batch(states, targets)\n",
    "\n",
    "    # update int model\n",
    "    int_loss = int_model.train_on_batch(states, states)\n",
    "\n",
    "    # soft update target Q\n",
    "    update_target(dqn, target_dqn, tau=tau)\n",
    "\n",
    "    return dqn_loss, int_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135196,
     "status": "ok",
     "timestamp": 1534482559393,
     "user": {
      "displayName": "윤승제",
      "photoUrl": "//lh5.googleusercontent.com/-EucKC7DmcQI/AAAAAAAAAAI/AAAAAAAAAGA/gQU1NPEmNFA/s50-c-k-no/photo.jpg",
      "userId": "105654037995838004821"
     },
     "user_tz": -540
    },
    "id": "PnifSBJglzHh",
    "outputId": "94177345-918e-4a96-d9a8-d8aba0a4bc9a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # make an environment\n",
    "        # env = gymnasium.make(\"CartPole-v1\")\n",
    "        env = gymnasium.make('MountainCar-v0')\n",
    "        # env = gymnasium.make(\"LunarLander-v2\")\n",
    "        # env = gymnasium.make('Acrobot-v1')\n",
    "\n",
    "        state_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "\n",
    "        # init variables\n",
    "        total_steps = 0\n",
    "        temp = config.temp_init\n",
    "\n",
    "        # init models\n",
    "        q_model = DuelingDQN(action_space)\n",
    "        q_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=config.lr,\n",
    "                global_clipnorm=config.global_clipnorm,\n",
    "            ),\n",
    "            loss=\"logcosh\",\n",
    "        )\n",
    "        target_q_model = DuelingDQN(action_space)\n",
    "        target_q_model.set_weights(q_model.get_weights())\n",
    "        int_model = IntrinsicModel()\n",
    "        int_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=config.lr,\n",
    "                global_clipnorm=config.global_clipnorm,\n",
    "            ),\n",
    "            loss=\"mse\",\n",
    "        )\n",
    "\n",
    "        # init buffer\n",
    "        exp_buffer = ReplayBuffer(shape=env.observation_space.shape, size=config.buffer_size)\n",
    "\n",
    "        # play\n",
    "        for epoch in range(0, config.epochs):\n",
    "            state, _ = env.reset()\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            done, truncated = False, False\n",
    "            ep_ext_reward, ep_int_reward, ep_step = 0, 0, 0\n",
    "\n",
    "            while (not done) and (not truncated):\n",
    "                action = q_model.get_action(state, temp)\n",
    "                action = np.array(action, copy=False, dtype=env.env.action_space.dtype)\n",
    "\n",
    "                # get intrinsic reward\n",
    "                int_reward = int_model.get_int_reward(state)[0, 0]\n",
    "\n",
    "                next_state, ext_reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                exp_buffer.add(state[0], action, ext_reward, int_reward, next_state, done)\n",
    "\n",
    "                state = np.expand_dims(next_state, axis=0)\n",
    "                total_steps += 1\n",
    "                ep_step += 1\n",
    "                ep_ext_reward += ext_reward\n",
    "                ep_int_reward += int_reward\n",
    "\n",
    "                # decrement temperature\n",
    "                temp -= config.temp_decay\n",
    "                temp = max(config.temp_min, temp)\n",
    "\n",
    "                if len(exp_buffer) >= config.batch_size:\n",
    "                    dqn_loss, int_loss = train_step(\n",
    "                        q_model,\n",
    "                        target_q_model,\n",
    "                        int_model,\n",
    "                        exp_buffer,\n",
    "                        config.batch_size,\n",
    "                        config.tau,\n",
    "                        config.gamma,\n",
    "                    )\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"dqn_loss\": dqn_loss,\n",
    "                            \"int_loss\": int_loss,\n",
    "                        },\n",
    "                        commit=False,\n",
    "                    )\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"score\": ep_ext_reward,\n",
    "                    \"int_score\": ep_int_reward,\n",
    "                    \"steps\": ep_step,\n",
    "                    \"temperature\": temp,\n",
    "                },\n",
    "                commit=True,\n",
    "                step=epoch,\n",
    "            )\n",
    "            print(\n",
    "                f\"Int reward - Mean: {int_model.rew_rms.mean}, Stddev: {tf.sqrt(int_model.rew_rms.variance)}, Count: {int_model.rew_rms.count}\"\n",
    "            )\n",
    "\n",
    "            # For 'Acrobot-v1' the threshold is -92\n",
    "            # For 'MountainCar-v0' the threshold is 'env.spec.reward_threshold'\n",
    "            # For 'CartPole-v1' the threshold is 'env.spec.reward_threshold'\n",
    "            # For 'LunarLander-v2' the threshold is 'env.spec.reward_threshold'\n",
    "            if ep_ext_reward > env.spec.reward_threshold:\n",
    "                q_model.summary()\n",
    "                target_q_model.summary()\n",
    "                int_model.summary()\n",
    "\n",
    "                print(\n",
    "                    f\"\\n{env.spec.id} is sloved! {(epoch+1)} Episode in {total_steps} steps with reward {ep_ext_reward}\"\n",
    "                )\n",
    "\n",
    "                # show latent space\n",
    "                latent_space = state_space // 2\n",
    "                sampled_states, _, _, _, _, _ = exp_buffer.sample(ep_step)\n",
    "                code_normal = int_model(sampled_states, only_encoder=True)\n",
    "                code_anomaly = int_model(\n",
    "                    exp_buffer.new_state_buffer[\n",
    "                        len(exp_buffer) - ep_step : len(exp_buffer)\n",
    "                    ],\n",
    "                    only_encoder=True,\n",
    "                )\n",
    "                if latent_space > 2:\n",
    "                    pca = PCA(n_components=2)\n",
    "                    code_normal = pca.fit_transform(code_normal)\n",
    "\n",
    "                    pca = PCA(n_components=2)\n",
    "                    code_anomaly = pca.fit_transform(code_anomaly)\n",
    "\n",
    "                    data = [\n",
    "                        [x, y, \"Normal\"]\n",
    "                        for (x, y) in zip(code_normal[:, 0], code_normal[:, 1])\n",
    "                    ]\n",
    "                    data += [\n",
    "                        [x, y, \"Winner\"]\n",
    "                        for (x, y) in zip(code_anomaly[:, 0], code_anomaly[:, 1])\n",
    "                    ]\n",
    "                elif latent_space == 2:\n",
    "                    data = [\n",
    "                        [x, y, \"Normal\"]\n",
    "                        for (x, y) in zip(code_normal[:, 0], code_normal[:, 1])\n",
    "                    ]\n",
    "                    data += [\n",
    "                        [x, y, \"Winner\"]\n",
    "                        for (x, y) in zip(code_anomaly[:, 0], code_anomaly[:, 1])\n",
    "                    ]\n",
    "                else:\n",
    "                    data = [\n",
    "                        [x, y, \"Normal\"]\n",
    "                        for (x, y) in zip(\n",
    "                            code_normal[:, 0], np.ones_like(code_normal[:, 0])\n",
    "                        )\n",
    "                    ]\n",
    "                    data += [\n",
    "                        [x, y, \"Winner\"]\n",
    "                        for (x, y) in zip(\n",
    "                            code_anomaly[:, 0], np.ones_like(code_anomaly[:, 0])\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                table = wandb.Table(data=data, columns=[\"LS 1\", \"LS 2\", \"Type\"])\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"Latent space\": wandb.plot.scatter(table, \"LS 1\", \"LS 2\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                break\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int reward - Mean: [[0.00072626]], Stddev: [[0.01423572]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=113200>\n",
      "Int reward - Mean: [[0.00072503]], Stddev: [[0.01422314]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=113400>\n",
      "Int reward - Mean: [[0.00072381]], Stddev: [[0.01421059]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=113600>\n",
      "Int reward - Mean: [[0.0007226]], Stddev: [[0.01419812]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=113800>\n",
      "Int reward - Mean: [[0.00072135]], Stddev: [[0.01418572]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=114000>\n",
      "Int reward - Mean: [[0.00072013]], Stddev: [[0.0141733]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=114200>\n",
      "Int reward - Mean: [[0.00071899]], Stddev: [[0.01416087]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=114400>\n",
      "Int reward - Mean: [[0.0007178]], Stddev: [[0.01414847]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=114600>\n",
      "Int reward - Mean: [[0.00071656]], Stddev: [[0.01413612]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=114800>\n",
      "Int reward - Mean: [[0.00071539]], Stddev: [[0.01412387]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=115000>\n",
      "Int reward - Mean: [[0.00071416]], Stddev: [[0.0141116]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=115200>\n",
      "Int reward - Mean: [[0.00071295]], Stddev: [[0.01409937]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=115400>\n",
      "Int reward - Mean: [[0.00071174]], Stddev: [[0.01408719]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=115600>\n",
      "Int reward - Mean: [[0.00071053]], Stddev: [[0.01407499]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=115800>\n",
      "Int reward - Mean: [[0.00070933]], Stddev: [[0.01406281]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=116000>\n",
      "Int reward - Mean: [[0.00070814]], Stddev: [[0.01405073]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=116200>\n",
      "Int reward - Mean: [[0.00070695]], Stddev: [[0.01403871]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=116400>\n",
      "Int reward - Mean: [[0.00070579]], Stddev: [[0.01402668]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=116600>\n",
      "Int reward - Mean: [[0.0007046]], Stddev: [[0.01401464]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=116800>\n",
      "Int reward - Mean: [[0.00070345]], Stddev: [[0.01400264]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=117000>\n",
      "Int reward - Mean: [[0.00070229]], Stddev: [[0.01399068]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=117200>\n",
      "Int reward - Mean: [[0.00070111]], Stddev: [[0.01397874]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=117400>\n",
      "Int reward - Mean: [[0.00069995]], Stddev: [[0.01396686]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=117600>\n",
      "Int reward - Mean: [[0.0006988]], Stddev: [[0.01395502]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=117800>\n",
      "Int reward - Mean: [[0.00069765]], Stddev: [[0.01394323]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=118000>\n",
      "Int reward - Mean: [[0.0006965]], Stddev: [[0.01393143]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=118200>\n",
      "Int reward - Mean: [[0.00069534]], Stddev: [[0.01391962]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=118400>\n",
      "Int reward - Mean: [[0.00069418]], Stddev: [[0.01390782]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=118600>\n",
      "Int reward - Mean: [[0.00069306]], Stddev: [[0.0138961]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=118800>\n",
      "Int reward - Mean: [[0.00069194]], Stddev: [[0.01388445]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=119000>\n",
      "Int reward - Mean: [[0.00069083]], Stddev: [[0.01387282]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=119200>\n",
      "Int reward - Mean: [[0.00068972]], Stddev: [[0.01386117]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=119400>\n",
      "Int reward - Mean: [[0.00068859]], Stddev: [[0.01384961]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=119600>\n",
      "Int reward - Mean: [[0.00068748]], Stddev: [[0.01383805]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=119800>\n",
      "Int reward - Mean: [[0.00068639]], Stddev: [[0.01382648]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=120000>\n",
      "Int reward - Mean: [[0.00068528]], Stddev: [[0.01381493]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=120200>\n",
      "Int reward - Mean: [[0.00068418]], Stddev: [[0.01380345]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=120400>\n",
      "Int reward - Mean: [[0.00068307]], Stddev: [[0.01379206]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=120600>\n",
      "Int reward - Mean: [[0.00068204]], Stddev: [[0.01378066]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=120800>\n",
      "Int reward - Mean: [[0.00068094]], Stddev: [[0.01376925]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=121000>\n",
      "Int reward - Mean: [[0.00067985]], Stddev: [[0.01375787]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=121200>\n",
      "Int reward - Mean: [[0.00067877]], Stddev: [[0.01374655]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=121400>\n",
      "Int reward - Mean: [[0.0006777]], Stddev: [[0.01373522]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=121600>\n",
      "Int reward - Mean: [[0.00067667]], Stddev: [[0.0137239]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=121800>\n",
      "Int reward - Mean: [[0.00067557]], Stddev: [[0.01371265]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=122000>\n",
      "Int reward - Mean: [[0.00067454]], Stddev: [[0.01370149]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=122200>\n",
      "Int reward - Mean: [[0.00067345]], Stddev: [[0.01369034]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=122400>\n",
      "Int reward - Mean: [[0.00067238]], Stddev: [[0.01367917]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=122600>\n",
      "Int reward - Mean: [[0.00067137]], Stddev: [[0.013668]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=122800>\n",
      "Int reward - Mean: [[0.00067029]], Stddev: [[0.01365686]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=123000>\n",
      "Int reward - Mean: [[0.00066929]], Stddev: [[0.01364578]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=123200>\n",
      "Int reward - Mean: [[0.00066824]], Stddev: [[0.01363468]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=123400>\n",
      "Int reward - Mean: [[0.00066718]], Stddev: [[0.01362365]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=123600>\n",
      "Int reward - Mean: [[0.00066615]], Stddev: [[0.01361264]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=123800>\n",
      "Int reward - Mean: [[0.00066512]], Stddev: [[0.01360172]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=124000>\n",
      "Int reward - Mean: [[0.00066411]], Stddev: [[0.01359081]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=124200>\n",
      "Int reward - Mean: [[0.00066305]], Stddev: [[0.01357988]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=124400>\n",
      "Int reward - Mean: [[0.000662]], Stddev: [[0.01356895]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=124600>\n",
      "Int reward - Mean: [[0.00066101]], Stddev: [[0.01355804]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=124800>\n",
      "Int reward - Mean: [[0.00065997]], Stddev: [[0.01354719]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=125000>\n",
      "Int reward - Mean: [[0.00065895]], Stddev: [[0.01353634]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=125200>\n",
      "Int reward - Mean: [[0.00065796]], Stddev: [[0.01352554]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=125400>\n",
      "Int reward - Mean: [[0.00065707]], Stddev: [[0.01351477]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=125600>\n",
      "Int reward - Mean: [[0.00065605]], Stddev: [[0.01350407]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=125800>\n",
      "Int reward - Mean: [[0.0006551]], Stddev: [[0.0134934]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=126000>\n",
      "Int reward - Mean: [[0.00065412]], Stddev: [[0.01348272]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=126200>\n",
      "Int reward - Mean: [[0.0006531]], Stddev: [[0.01347203]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=126400>\n",
      "Int reward - Mean: [[0.00065212]], Stddev: [[0.01346133]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=126600>\n",
      "Int reward - Mean: [[0.00065112]], Stddev: [[0.01345072]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=126800>\n",
      "Int reward - Mean: [[0.00065015]], Stddev: [[0.01344012]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=127000>\n",
      "Int reward - Mean: [[0.00064913]], Stddev: [[0.0134295]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=127200>\n",
      "Int reward - Mean: [[0.00064812]], Stddev: [[0.01341898]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=127400>\n",
      "Int reward - Mean: [[0.00064714]], Stddev: [[0.01340846]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=127600>\n",
      "Int reward - Mean: [[0.00064618]], Stddev: [[0.01339804]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=127800>\n",
      "Int reward - Mean: [[0.00064519]], Stddev: [[0.01338761]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=128000>\n",
      "Int reward - Mean: [[0.00064423]], Stddev: [[0.01337717]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=128200>\n",
      "Int reward - Mean: [[0.00064325]], Stddev: [[0.01336672]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=128400>\n",
      "Int reward - Mean: [[0.00064228]], Stddev: [[0.01335629]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=128600>\n",
      "Int reward - Mean: [[0.00064132]], Stddev: [[0.01334593]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=128800>\n",
      "Int reward - Mean: [[0.00064036]], Stddev: [[0.01333557]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=129000>\n",
      "Int reward - Mean: [[0.00063941]], Stddev: [[0.0133252]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=129200>\n",
      "Int reward - Mean: [[0.00063844]], Stddev: [[0.0133149]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=129400>\n",
      "Int reward - Mean: [[0.00063748]], Stddev: [[0.01330462]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=129600>\n",
      "Int reward - Mean: [[0.00063655]], Stddev: [[0.01329442]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=129800>\n",
      "Int reward - Mean: [[0.0006356]], Stddev: [[0.01328424]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=130000>\n",
      "Int reward - Mean: [[0.00063467]], Stddev: [[0.01327405]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=130200>\n",
      "Int reward - Mean: [[0.00063372]], Stddev: [[0.01326385]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=130400>\n",
      "Int reward - Mean: [[0.0006328]], Stddev: [[0.01325366]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=130600>\n",
      "Int reward - Mean: [[0.00063187]], Stddev: [[0.01324356]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=130800>\n",
      "Int reward - Mean: [[0.00063093]], Stddev: [[0.01323344]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=131000>\n",
      "Int reward - Mean: [[0.00063]], Stddev: [[0.01322332]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=131200>\n",
      "Int reward - Mean: [[0.00062905]], Stddev: [[0.01321319]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=131400>\n",
      "Int reward - Mean: [[0.00062813]], Stddev: [[0.01320314]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=131600>\n",
      "Int reward - Mean: [[0.00062723]], Stddev: [[0.01319311]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=131800>\n",
      "Int reward - Mean: [[0.00062631]], Stddev: [[0.01318314]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=132000>\n",
      "Int reward - Mean: [[0.00062539]], Stddev: [[0.0131732]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=132200>\n",
      "Int reward - Mean: [[0.00062446]], Stddev: [[0.01316326]], Count: <tf.Variable 'intrinsic_model/count:0' shape=() dtype=int64, numpy=132400>\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(\"Dueling-DQN-with-AutoEncoder/x206qsz8\", run, count=74)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C51_tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f8d278994ad275c52e282a0ce8b95e2077469e5ea30fad259438b075059602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
